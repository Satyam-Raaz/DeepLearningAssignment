{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.1>  How do you create a simple perceptron for basic binary classification?\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "model=Sequential([\n",
    "    Dense(64,activation='relu',input_shape=(10,)),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.2>  How can you build a neural network with one hidden layer using Keras?\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "model=Sequential([\n",
    "    Dense(64,activation='relu',input_shape=(10,)),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Q.No.3>  How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.initializers import GlorotNormal\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "#First method to using gloral_normal() function\n",
    "model=Sequential([\n",
    "    Dense(64,kernel_initializer=GlorotNormal,activation='relu',input_shape=(10,)),\n",
    "    Dense(32,kernel_initializer=GlorotNormal,activation='relu'),\n",
    "    Dense(1,kernel_initializer=GlorotNormal,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "\n",
    "#First method to using gloral_uniform() function\n",
    "model=Sequential([\n",
    "    Dense(64,kernel_initializer= GlorotUniform,activation='relu',input_shape=(10,)),\n",
    "    Dense(32,kernel_initializer= GlorotUniform,activation='relu'),\n",
    "    Dense(1,kernel_initializer= GlorotUniform,activation='sigmoid'),\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.4>  How can you apply different activation functions in a neural network in Keras?\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "model=Sequential([\n",
    "    Dense(64,activation='relu',input_shape=(10,)),\n",
    "    Dense(32,activation='softmax'),\n",
    "    Dense(32,activation='tanh'),\n",
    "    Dense(32,activation='swish'),\n",
    "    Dense(1,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Q.No.5>  How do you add dropout to a neural network model to prevent overfitting?\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "model=Sequential([\n",
    "    Dense(128,activation='relu',input_shape=(10,)),\n",
    "    Dropout(.2),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dropout(.3),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dropout(.1),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.6> How do you manually implement forward propagation in a simple neural network?\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example input (2 features)\n",
    "x = np.array([0.5, 0.3])\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.array([[0.1, 0.2], [0.3, 0.4]])  # weights for input to hidden layer\n",
    "b1 = np.array([0.1, 0.1])  # bias for hidden layer\n",
    "W2 = np.array([0.5, 0.6])  # weights for hidden to output layer\n",
    "b2 = 0.2  # bias for output layer\n",
    "\n",
    "# Step 1: Compute hidden layer activations\n",
    "z1 = np.dot(x, W1) + b1  # Weighted sum for hidden layer\n",
    "a1 = sigmoid(z1)  # Activation for hidden layer\n",
    "\n",
    "# Step 2: Compute output layer activation\n",
    "z2 = np.dot(a1, W2) + b2  # Weighted sum for output layer\n",
    "a2 = sigmoid(z2)  # Activation for output layer\n",
    "\n",
    "# Final output\n",
    "print(\"Output:\", a2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.7>  How do you add batch normalization to a neural network model in Keras \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "model=Sequential([\n",
    "    Dense(128,activation='relu',input_shape=(10,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(32,activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(.3),\n",
    "    Dense(32,activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.8>  How can you visualize the training process with accuracy and loss curves?\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "data=load_iris()\n",
    "x=data.data\n",
    "y=data.target\n",
    "model=Sequential([\n",
    "    Dense(128,activation='relu',input_shape=(4,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(32,activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(.3),\n",
    "    Dense(32,activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history=model.fit(x,y,epochs=25,batch_size=32,validation_split=0.2)\n",
    "\n",
    "history\n",
    "\n",
    "#plotting accuracy and loss during training\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.plot(history.history['accuracy'],label='Training Accuracy')\n",
    "plt.plot(history.history['loss'],label='Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.9>  How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=10),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with gradient clipping by value\n",
    "optimizer = Adam(clipvalue=1.0) \n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.10>  How can you create a custom loss function in Keras?\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define custom loss function\n",
    "def custom_mse(y_true, y_pred):\n",
    "    # Calculate the mean squared error\n",
    "    mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "    # Add a small constant to avoid division by zero (optional)\n",
    "    loss = mse + 0.01\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "#Q.No.11>  How can you visualize the structure of a neural network model in Keras?\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "model=Sequential([\n",
    "    Dense(64,activation='relu',input_shape=(10,)),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1,activation='sigmoid'),\n",
    "\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "plot_model(model,to_file='simple_neural_network.png',show_shapes=True,show_layers_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
