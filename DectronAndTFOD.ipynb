{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.1> How do you install Detectron2 using pip and check the version of Detectron2?\n",
    "%pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch1.13/index.html\n",
    "\n",
    "import detectron2\n",
    "print(detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.2> How do you perform inference with Detectron2 using an online image?\n",
    "\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Step 1: Download the image from a URL\n",
    "url = \"https://wallup.net/wp-content/uploads/2016/01/211594-nature-landscape.jpg\"  # Replace with your image URL\n",
    "response = requests.get(url)\n",
    "image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Step 2: Set up the Detectron2 configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.DEVICE = \"cuda\"  # Use \"cpu\" if you don't have a GPU\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Step 3: Perform inference\n",
    "outputs = predictor(image)\n",
    "\n",
    "# Step 4: Visualize the results\n",
    "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "result_image = v.get_image()[:, :, ::-1]\n",
    "\n",
    "# Step 5: Display the output image\n",
    "cv2.imshow(\"Inference Result\", result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.3>How do you visualize evaluation metrics in Detectron2, such as training loss?\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import tensorboard\n",
    "\n",
    "# Step 1: Install and Import Required Libraries\n",
    "# Install the required libraries:\n",
    "# pip install detectron2 requests opencv-python tensorboard\n",
    "\n",
    "# Step 2: Set up configuration and model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.DEVICE = \"cuda\"  # Use \"cpu\" if no GPU available\n",
    "\n",
    "# Step 3: Define output directory for logging\n",
    "cfg.OUTPUT_DIR = \"./output\"  # Directory where logs will be saved\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Step 4: Set up trainer and start training\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "# Training the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 5: Visualizing with TensorBoard\n",
    "# After training starts, open a terminal and run the following command:\n",
    "# tensorboard --logdir=output\n",
    "# Then open your browser and navigate to http://localhost:6006 to view the training logs and metrics.\n",
    "\n",
    "# Step 6: Perform inference with an online image\n",
    "# Download an image from a URL and perform inference\n",
    "url = \"https://wallup.net/wp-content/uploads/2016/01/211594-nature-landscape.jpg\"  # Replace with your image URL\n",
    "response = requests.get(url)\n",
    "image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Initialize the model with the trained weights for inference\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Perform inference\n",
    "outputs = predictor(image)\n",
    "\n",
    "# Visualize the results\n",
    "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "result_image = v.get_image()[:, :, ::-1]\n",
    "\n",
    "# Display the inference result\n",
    "cv2.imshow(\"Inference Result\", result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.4> How do you run inference with TFOD2 on an online image?\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import cv2\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from io import BytesIO\n",
    "\n",
    "# Step 1: Load the Pretrained Model\n",
    "# Load the model from a pre-trained model checkpoint (replace with your custom model path if needed)\n",
    "MODEL_NAME = 'ssd_inception_v2_coco_2017_11_17'  # Example pre-trained model\n",
    "PATH_TO_CKPT = f'http://download.tensorflow.org/models/object_detection/{MODEL_NAME}.tar.gz'\n",
    "\n",
    "# Download and extract the model (for online access)\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "response = requests.get(PATH_TO_CKPT)\n",
    "with open('model.tar.gz', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "with tarfile.open('model.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./')\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.saved_model.load('./ssd_inception_v2_coco_2017_11_17/saved_model')\n",
    "\n",
    "# Step 2: Load Label Map\n",
    "LABEL_MAP_PATH = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True)\n",
    "\n",
    "# Step 3: Function to Load and Prepare the Image\n",
    "def load_image_into_numpy_array(url):\n",
    "    response = requests.get(url)\n",
    "    image_data = np.array(bytearray(response.content), dtype=np.uint8)\n",
    "    image = cv2.imdecode(image_data, cv2.IMREAD_COLOR)  # Decode image into an array\n",
    "    return image\n",
    "\n",
    "# Step 4: Run Inference\n",
    "def run_inference(image_path):\n",
    "    image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "    # The input needs to be a tensor, so we convert the image to a tensor\n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    input_tensor = input_tensor[tf.newaxis,...]  # Add batch dimension\n",
    "\n",
    "    # Run detection\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches of detections, so we take the first one.\n",
    "    output_dict = {key:value.numpy() for key,value in output_dict.items()}\n",
    "\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'][0],\n",
    "        output_dict['detection_classes'][0].astype(np.int32),\n",
    "        output_dict['detection_scores'][0],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks', None),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "\n",
    "    return image_np\n",
    "\n",
    "# Step 5: Visualize Results\n",
    "# Example: URL of an online image\n",
    "image_url = 'https://example.com/your_image.jpg'  # Replace with your image URL\n",
    "\n",
    "result_image = run_inference(image_url)\n",
    "\n",
    "# Step 6: Display the result\n",
    "cv2.imshow('Detection Result', result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.5> How do you install TensorFlow Object Detection API in Jupyter Notebook?\n",
    "# Install TensorFlow\n",
    "!pip install tensorflow\n",
    "\n",
    "# Install dependencies for TensorFlow Object Detection API\n",
    "!pip install tf-slim\n",
    "!pip install tensorflow-hub\n",
    "!pip install tensorflow-graphics\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "\n",
    "# Install the Object Detection API\n",
    "!pip install --upgrade pip\n",
    "!pip install setuptools==59.5.0\n",
    "!pip install tensorflow-object-detection-api\n",
    "# Install dependencies for building TensorFlow Object Detection API from source\n",
    "!pip install pillow lxml Cython contextlib2 jupyter\n",
    "!pip install matplotlib pandas opencv-python tf-slim\n",
    "\n",
    "# Clone the TensorFlow models repository (if not already available)\n",
    "!git clone https://github.com/tensorflow/models.git\n",
    "\n",
    "# Navigate to the 'models' directory\n",
    "%cd models/research/\n",
    "\n",
    "# Install the Object Detection API from source\n",
    "!python setup.py install\n",
    "\n",
    "\n",
    "# Test the installation by importing the Object Detection API\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Object Detection API is installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.6>  How can you load a pre-trained TensorFlow Object Detection model?\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import model_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# URL of the pre-trained model from TensorFlow Model Zoo\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "MODEL_PATH = 'http://download.tensorflow.org/models/object_detection/' + MODEL_NAME + '.tar.gz'\n",
    "\n",
    "# Download and extract the model\n",
    "import tarfile\n",
    "import os\n",
    "import requests\n",
    "\n",
    "response = requests.get(MODEL_PATH)\n",
    "with open('model.tar.gz', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the downloaded tar file\n",
    "with tarfile.open('model.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./')\n",
    "\n",
    "# Get the path to the saved_model directory\n",
    "PATH_TO_SAVED_MODEL = './ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model'\n",
    "# Load the saved model\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "# Load label map for COCO\n",
    "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "# Load an image from file (replace with the image path or URL)\n",
    "image_path = 'your_image.jpg'  # Replace with your image file path\n",
    "image_np = cv2.imread(image_path)\n",
    "image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "# Convert image to tensor\n",
    "input_tensor = tf.convert_to_tensor(image_np)\n",
    "input_tensor = input_tensor[tf.newaxis,...]  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "output_dict = detect_fn(input_tensor)\n",
    "\n",
    "# The output dictionary contains:\n",
    "# 'detection_boxes', 'detection_scores', 'detection_classes', 'num_detections'\n",
    "output_dict = {key:value.numpy() for key,value in output_dict.items()}\n",
    "\n",
    "# Visualize the results\n",
    "vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "    image_np,\n",
    "    output_dict['detection_boxes'][0],\n",
    "    output_dict['detection_classes'][0].astype(np.int32),\n",
    "    output_dict['detection_scores'][0],\n",
    "    category_index,\n",
    "    instance_masks=output_dict.get('detection_masks', None),\n",
    "    use_normalized_coordinates=True,\n",
    "    line_thickness=8\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Detection Result', cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.7> How do you preprocess an image from the web for TFOD2 inference?\n",
    "%pip install tensorflow opencv-python numpy requests\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import cv2\n",
    "\n",
    "def preprocess_image_from_web(image_url, target_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess an image from a URL for TFOD2 inference.\n",
    "\n",
    "    Args:\n",
    "        image_url (str): The URL of the image to download and preprocess.\n",
    "        target_size (tuple): Target size for resizing the image, e.g., (320, 320). If None, no resizing is done.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Preprocessed image tensor with batch dimension.\n",
    "        np.array: Original image as a NumPy array (RGB format) for visualization.\n",
    "    \"\"\"\n",
    "    # Step 1: Download the image\n",
    "    response = requests.get(image_url)\n",
    "    image_data = np.array(bytearray(response.content), dtype=np.uint8)\n",
    "\n",
    "    # Step 2: Decode the image\n",
    "    image = cv2.imdecode(image_data, cv2.IMREAD_COLOR)  # Decode to BGR format\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)      # Convert to RGB format\n",
    "\n",
    "    # Step 3: Resize the image (if target_size is provided)\n",
    "    if target_size:\n",
    "        image = cv2.resize(image, target_size)\n",
    "\n",
    "    # Step 4: Normalize pixel values (if required by your model)\n",
    "    # image = image / 255.0  # Uncomment if the model requires normalization\n",
    "\n",
    "    # Step 5: Convert to TensorFlow tensor and add batch dimension\n",
    "    input_tensor = tf.convert_to_tensor(image, dtype=tf.uint8)  # Use dtype=tf.float32 if normalized\n",
    "    input_tensor = tf.expand_dims(input_tensor, axis=0)  # Add batch dimension\n",
    "\n",
    "    return input_tensor, image\n",
    "\n",
    "# Example usage:\n",
    "image_url = 'https://wallup.net/wp-content/uploads/2016/01/211594-nature-landscape.jpg'  # Replace with your image URL\n",
    "target_size = (320, 320)  # Replace with the size required by your model\n",
    "\n",
    "input_tensor, original_image = preprocess_image_from_web(image_url, target_size)\n",
    "\n",
    "print(f\"Preprocessed Tensor Shape: {input_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.8> How do you visualize bounding boxes for detected objects in TFOD2 inference?\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "def visualize_detections(image, output_dict, category_index):\n",
    "    \"\"\"\n",
    "    Visualizes bounding boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The original image in RGB format.\n",
    "        output_dict (dict): Model output containing detection boxes, classes, and scores.\n",
    "        category_index (dict): Dictionary mapping class IDs to class names.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'].astype(np.int32),\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=5\n",
    "    )\n",
    "    return image\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example detection output (replace with actual output from model inference)\n",
    "    output_dict = {\n",
    "        'detection_boxes': np.array([[0.1, 0.1, 0.5, 0.5], [0.6, 0.6, 0.9, 0.9]]),  # [ymin, xmin, ymax, xmax]\n",
    "        'detection_classes': np.array([1, 3]),  # Class IDs (e.g., person, car, etc.)\n",
    "        'detection_scores': np.array([0.95, 0.8])  # Confidence scores\n",
    "    }\n",
    "\n",
    "    # Load an image (replace with your image path)\n",
    "    image_path = 'example_image.jpg'  # Replace with the actual image path\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    # Load category index (e.g., COCO label map)\n",
    "    label_map_path = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "    category_index = label_map_util.create_category_index_from_labelmap(label_map_path, use_display_name=True)\n",
    "\n",
    "    # Visualize detections\n",
    "    image_with_detections = visualize_detections(image, output_dict, category_index)\n",
    "\n",
    "    # Display the image with detections\n",
    "    image_with_detections = cv2.cvtColor(image_with_detections, cv2.COLOR_RGB2BGR)  # Convert back to BGR for OpenCV\n",
    "    cv2.imshow('Detections', image_with_detections)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.9> How do you define classes for custom training in TFOD2?\n",
    "import tensorflow as tf\n",
    "\n",
    "def resize_image_tf(image, target_size):\n",
    "    \"\"\"\n",
    "    Resize an image to the specified dimensions using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        image (tf.Tensor): Input image tensor in the format [height, width, channels].\n",
    "        target_size (tuple): Target size as (height, width).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Resized image tensor.\n",
    "    \"\"\"\n",
    "    resized_image = tf.image.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "# Example Usage\n",
    "image = tf.random.uniform(shape=(480, 640, 3), minval=0, maxval=255, dtype=tf.float32)  # Dummy image\n",
    "target_size = (320, 320)\n",
    "resized_image = resize_image_tf(image, target_size)\n",
    "print(f\"Resized Image Shape: {resized_image.shape}\")\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.11> EE> How do you resize an image before detecting object1\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def resize_image_tf(image, target_size):\n",
    "    \"\"\"\n",
    "    Resize an image to the specified dimensions using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        image (tf.Tensor): Input image tensor in the format [height, width, channels].\n",
    "        target_size (tuple): Target size as (height, width).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Resized image tensor.\n",
    "    \"\"\"\n",
    "    resized_image = tf.image.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "# Example Usage\n",
    "image = tf.random.uniform(shape=(480, 640, 3), minval=0, maxval=255, dtype=tf.float32)  # Dummy image\n",
    "target_size = (320, 320)\n",
    "resized_image = resize_image_tf(image, target_size)\n",
    "print(f\"Resized Image Shape: {resized_image.shape}\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.12>  How can you apply a color filter (e.g., red filter) to an image?\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def apply_red_filter(image):\n",
    "    \"\"\"\n",
    "    Applies a red filter to an image by enhancing the red channel.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with a red filter applied.\n",
    "    \"\"\"\n",
    "    # Split the image into its color channels (B, G, R)\n",
    "    blue, green, red = cv2.split(image)\n",
    "    \n",
    "    # Set the blue and green channels to zero\n",
    "    blue[:] = 0\n",
    "    green[:] = 0\n",
    "    \n",
    "    # Merge the channels back\n",
    "    red_filtered_image = cv2.merge((blue, green, red))\n",
    "    return red_filtered_image\n",
    "\n",
    "# Example Usage\n",
    "image = cv2.imread('example_image.jpg')  # Load image\n",
    "red_filtered_image = apply_red_filter(image)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Red Filtered Image', red_filtered_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
